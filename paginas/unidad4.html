<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unidad 4</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Montserrat+Alternates:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Prompt:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="/css/styles.css">
</head>

<body>
    <header>
        <a href="/index.html"> <h1>Arquitectura de Computadoras</h1></a>
        <nav>
            <ul>
                <li><a href="unidad1.html">Unidad 1</a></li>
                <li><a href="unidad2.html">Unidad 2</a></li>
                <li><a href="unidad3.html">Unidad 3</a></li>
                <li><a href="#" class="activo">Unidad 4</a></li>
                <li><a href="practicas.html">Practicas</a></li>

            </ul>
        </nav>

    </header>
    <br><br>

    <div class="container">
        <aside>
            <h2>Temario Unidad 4</h2>
            <ul>
                <li>4.1 Aspectos básicos de la computación paralela</li>
                <li>4.2 Tipos de computación paralela</li>
                <li>4.2.1 Clasificación</li>
                <li>4.2.2 Arquitectura de computadores secuenciales</li>
                <li>4.2.3 Organización de direcciones de memoria</li>
                <li>4.3 Sistema de memoria compartida</li>
                <li>4.3.1.1 Redes de medio compartida</li>
                <li>4.3.1.2 Redes conmutadas</li>
                <li>4.4 Sistemas de memoria construida</li>
                <li>4.5 Casos de estudio</li>
            </ul>
        </aside>
        <main>
            <article>
                <h3>
                    4.1 Aspectos básicos de la computación paralela
                </h3>
                <p>

                    <img src="../img/Paralela.jpg" alt="">
                    La computación paralela se basa en la idea de dividir un problema en tareas más pequeñas y
                    procesarlas de
                    manera simultánea utilizando múltiples recursos de computación. Esto permite un procesamiento más
                    rápido y
                    eficiente en comparación con los enfoques secuenciales tradicionales. Algunos aspectos fundamentales
                    de la
                    computación paralela incluyen la sincronización de tareas, la comunicación entre procesos y la
                    gestión de
                    recursos.


                </p>
            </article>
            <article>
                <h3>4.2 Tipos de computación paralela</h3>
                <p>
                    <img src="../img/paralelaaa.jpg" alt="">
                    Existen varios tipos de computación paralela que se utilizan en diferentes contextos y escenarios.
                    Algunos de
                    los enfoques más comunes incluyen el procesamiento paralelo a nivel de bit, a nivel de instrucción,
                    a nivel de
                    datos y a nivel de tarea. Estos enfoques se diferencian en cómo se dividen y procesan las tareas y
                    los datos.
                </p>
            </article>
            <article>
                <h3>4.2.1 Clasificación</h3>
                <p>
                    <img src="../img/papa.jpg" alt="">
                    La clasificación de la computación paralela puede realizarse en función de la forma en que se
                    dividen las
                    tareas y los datos, así como de la forma en que se coordinan y comunican los procesos paralelos.
                    Algunas
                    clasificaciones comunes incluyen la computación paralela a nivel de bit, a nivel de instrucción, a
                    nivel de
                    datos y a nivel de tarea.
                </p>
            </article>
            <article>
                <h3>4.2.2 Arquitectura de computadores secuenciales</h3>
                <p>
                    <img src="../img/seuc.png" alt="">
                    La arquitectura de computadores secuencial se refiere a los sistemas informáticos tradicionales en
                    los que las
                    instrucciones se ejecutan una tras otra en secuencia. Este tipo de arquitectura sigue siendo común
                    en muchas
                    computadoras personales y estaciones de trabajo.
                </p>
            </article>
            <article>
                <h3>4.2.3 Organización de direcciones de memoria</h3>
                <p>
                    <img src="../img/memor.gif" alt="">
                    La organización de direcciones de memoria se refiere a cómo se asignan y acceden a las direcciones
                    de memoria
                    en un sistema de computación paralela. Esto incluye consideraciones como la memoria compartida, la
                    memoria
                    distribuida y las técnicas de direccionamiento utilizadas para acceder a los datos en paralelo.
                </p>
            </article>
            <article>
                <h3>4.3 Sistema de memoria compartida</h3>
                <p>
                    <img src="../img/redes.jpeg" alt="">
                    Los sistemas de memoria compartida son un enfoque de computación paralela en el que múltiples
                    procesadores
                    acceden a una misma área de memoria compartida. Esto permite a los procesadores compartir datos y
                    comunicarse
                    de manera eficiente. Dentro de los sistemas de memoria compartida, existen dos tipos principales de
                    redes: las
                    redes de medio compartida y las redes conmutadas.
                </p>
            </article>
            <article>
                <h3>4.3.1.1 Redes de medio compartida</h3>
                <p>
                    <img src="../img/redes2.jpg" alt="">
                    Las redes de medio compartida son un tipo de arquitectura de memoria compartida en la que los
                    procesadores se
                    conectan físicamente a un bus compartido o a una red de interconexión. Los procesadores pueden leer
                    y escribir
                    en la memoria compartida a través de este medio compartido.
                </p>
            </article>
            <article>
                <h3>4.3.1.2 Redes conmutadas</h3>
                <p>
                    <img src="../img/redes3.png" alt="">
                    Las redes conmutadas, por otro lado, utilizan interruptores o conmutadores para establecer
                    conexiones entre
                    los procesadores y la memoria compartida. Estas redes ofrecen una mayor escalabilidad y capacidad de
                    comunicación en comparación con las redes de medio compartida.
                </p>
            </article>
            <article>
                <h3>4.4 Sistemas de memoria construida</h3>
                <p>
                    Los sistemas de memoria construida son una forma de organización de la memoria en la computación
                    paralela en
                    la que cada procesador tiene su propia memoria local. Esto permite una mayor independencia entre los
                    procesadores y reduce la necesidad de acceder a una memoria compartida.
                </p>
            </article>
            <article>
                <h3>4.5 Casos de estudio</h3>
                <p>
                    En el campo de la computación paralela, existen numerosos casos de estudio que han demostrado la
                    eficacia y
                    los beneficios de los enfoques paralelos en diferentes dominios. Algunos ejemplos incluyen el uso de
                    computación paralela en simulaciones científicas, análisis de grandes conjuntos de datos,
                    renderizado de
                    gráficos y modelado de sistemas complejos.
                </p>
            </article>

        </main>
    </div>
    <style>
        img {
            width: 300px;
            height: 200px;
            border-radius: 10px;
            margin: 10px;
            float: right;
        }
    </style>
</body>

</html>